{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad de O'Higgins\n",
        "\n",
        "## Escuela de Ingeniería\n",
        "## COM4402: Introducción a Inteligencia Artificial\n",
        "\n",
        "### **Tarea 1: Clustering & Reducción de Dimensionalidad**\n",
        "\n",
        "### Estudiante: Bastián Rubio Moya\n",
        "\n",
        "\n",
        "El objetivo de esta tarea es utilizar distintos algoritmos de clustering y analizar su desempeño. Se utilizará la base de datos Mice Protein Expression Data Set, que es una base de datos tomadas del UC Irvine Machine Learning Repository. Contiene niveles de expresión de 77 proteínas medidas en fracciones nucleares de células en la corteza cerebral. Hay 38 ratones de control y 34 ratones trisómicos (síndrome de Down). Dado que algunos ratones fueron estimulados para aprender y/o inyectados con memantina, hay 8 grupos en total.\n",
        "\n",
        "Se pide utilizar tres algoritmos de clustering en este trabajo: K-Means, DBSCAN y Clustering aglomerativo."
      ],
      "metadata": {
        "id": "h6pEqigF7xMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En los Pasos 0.1 y 0.2, solo debe ejecutar las celdas respectivas"
      ],
      "metadata": {
        "id": "F6Kuz_Tw9YnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 0.1: Importación de librerías"
      ],
      "metadata": {
        "id": "UK8APusB8BTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Para evitar problemas de versión con xlrd\n",
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "id": "A_XEVbFO8E8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26530bf-9e2d-40bd-c655-4b7a2bda9670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 0.2: Definición de funciones auxiliares"
      ],
      "metadata": {
        "id": "mZsKlfEx8YDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiWkOj5F7kq5"
      },
      "outputs": [],
      "source": [
        "def bench_k_means(kmeans, name, data, labels):\n",
        "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    kmeans : KMeans instance\n",
        "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
        "        already set.\n",
        "    name : str\n",
        "        Name given to the strategy. It will be used to show the results in a\n",
        "        table.\n",
        "    data : ndarray of shape (n_samples, n_features)\n",
        "        The data to cluster.\n",
        "    labels : ndarray of shape (n_samples,)\n",
        "        The labels used to compute the clustering metrics which requires some\n",
        "        supervision.\n",
        "    \"\"\"\n",
        "    t0 = time()\n",
        "    estimator = kmeans.fit(data)\n",
        "    fit_time = time() - t0\n",
        "    results = [name, estimator.n_clusters, fit_time, estimator.inertia_]\n",
        "\n",
        "    # Define the metrics which require only the true labels and estimator\n",
        "    # labels\n",
        "    clustering_metrics = [\n",
        "        metrics.homogeneity_score,\n",
        "        metrics.completeness_score,\n",
        "        metrics.v_measure_score,\n",
        "        metrics.adjusted_rand_score,\n",
        "        metrics.adjusted_mutual_info_score,\n",
        "    ]\n",
        "    results += [m(labels, estimator.labels_) for m in clustering_metrics]\n",
        "\n",
        "    # The silhouette score requires the full dataset\n",
        "    results += [\n",
        "        metrics.silhouette_score(\n",
        "            data,\n",
        "            estimator.labels_,\n",
        "            metric=\"euclidean\",\n",
        "            #sample_size=300, # No se usa, para que el calculo sea parejo respecto a bench_clustering_2\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Show the results\n",
        "    formatter_result = (\n",
        "        \"{:9s}\\t{:.0f}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
        "    )\n",
        "    print(formatter_result.format(*results))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bench_clustering2(estim, name, data, labels, use_outliers= False):\n",
        "    \"\"\"Benchmark to evaluate the DBSCAN and AgglomerativeClustering initialization methods.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estim : DBSCAN or AGC instance\n",
        "        A :class:`~sklearn.cluster.DBSCAN` or  `~sklearn.cluster.AgglomerativeClustering` instance with the initialization\n",
        "        already set.\n",
        "    name : str\n",
        "        Name given to the strategy. It will be used to show the results in a\n",
        "        table.\n",
        "    data : ndarray of shape (n_samples, n_features)\n",
        "        The data to cluster.\n",
        "    labels : ndarray of shape (n_samples,)\n",
        "        The labels used to compute the clustering metrics which requires some\n",
        "        supervision.\n",
        "    \"\"\"\n",
        "    t0 = time()\n",
        "    estimator = estim.fit(data)\n",
        "    fit_time = time() - t0\n",
        "\n",
        "\n",
        "    e_labels = estimator.labels_\n",
        "\n",
        "    # Ni DBSCAN ni Agglomerative poseen la variable inertia_., DBSCAN no posee la variable n_clusters\n",
        "    try:\n",
        "      n_clusters = estimator.n_clusters\n",
        "      results = [name, n_clusters, fit_time, -1]\n",
        "    except:\n",
        "      # Es dbscan (falla en n_clusters)\n",
        "\n",
        "      if use_outliers: # Se usan los outliers\n",
        "        n_clusters = len(set(e_labels))\n",
        "        results = [name, n_clusters, fit_time, -1]\n",
        "\n",
        "      else: # No se usan los outliers\n",
        "        n_clusters = len(set(e_labels))- (1 if -1 in e_labels else 0)\n",
        "        results = [name, n_clusters, fit_time, -1]\n",
        "\n",
        "        # Sólo se usan los datos que no contemplan outliers\n",
        "        indexs = np.argwhere(e_labels!=-1)\n",
        "        if len(indexs)>1:\n",
        "          indexs = np.concatenate(indexs)\n",
        "          data = data[indexs,]\n",
        "          labels = labels[indexs,]\n",
        "          e_labels = e_labels[indexs,]\n",
        "\n",
        "\n",
        "    # Define the metrics which require only the true labels and estimator\n",
        "    # labels\n",
        "    clustering_metrics = [\n",
        "        metrics.homogeneity_score,\n",
        "        metrics.completeness_score,\n",
        "        metrics.v_measure_score,\n",
        "        metrics.adjusted_rand_score,\n",
        "        metrics.adjusted_mutual_info_score,\n",
        "    ]\n",
        "    # Si hay más de un cluster\n",
        "    if n_clusters>1:\n",
        "      results += [m(labels, e_labels) for m in clustering_metrics]\n",
        "\n",
        "      # The silhouette score requires the full dataset\n",
        "      results += [\n",
        "          metrics.silhouette_score(\n",
        "              data,\n",
        "              e_labels,\n",
        "              metric=\"euclidean\",\n",
        "              #sample_size= None, # No se usa ya que puede dar errores si la muestra extraída solamente posee un solo label\n",
        "          )\n",
        "      ]\n",
        "    else:\n",
        "      results += [-1 for i in range(len(clustering_metrics)+1)]\n",
        "\n",
        "    # Show the results\n",
        "    formatter_result = (\n",
        "        \"{:9s}\\t{:.0f}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
        "    )\n",
        "    print(formatter_result.format(*results))"
      ],
      "metadata": {
        "id": "CT-z1N4F8jug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## De aquí en adelante empezará su implementación, la cuál será guiada"
      ],
      "metadata": {
        "id": "9OaUoU4u8uTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conexión a drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt78NRzfwmEd",
        "outputId": "63cc60a7-d554-4e2f-e034-e2cae176d8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ignorar warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Zky99KITtyYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1\n",
        "\n",
        "Implemente un código que lea la base de datos \"data_cortex_nuclear.csv\" proporcionada en U-Campus. Se recomienda usar la biblioteca Pandas.\n",
        "\n",
        "Se debe eliminar la columna BCL2_N, dado que contiene muchos valores NaN. Asimismo, las filas que contienen valores NaN deben ser eliminadas. Finalmente, para esta tarea, se recomienda no usar scale( ) sobre los datos."
      ],
      "metadata": {
        "id": "9hvcbTfl9Flw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente en esta celda un codigo en Pandas que lea la base de datos \"data_cortex_nuclear.csv\". Hint: busque sobre pandas.ExcelFile\n",
        "\n",
        "#Cargar data desde drive\n",
        "\n",
        "data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/IA/Tarea 1/Data_Cortex_Nuclear.xls') #Modificar url en caso de ser necesario\n",
        "data = data.drop(columns=['MouseID','BCL2_N', 'Genotype', 'Treatment', 'Behavior'])\n",
        "data = data.dropna() #Se eliminan las filas con valores NaN"
      ],
      "metadata": {
        "id": "1ZEzpp8W901d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2 & 3\n",
        "\n",
        "Entrenar los siguientes algoritmos de clustering, con los parámetros especificados:\n",
        "    \n",
        "*   K-Means (con inicialización al azar), usando 8 clusters\n",
        "*   K-Means++, usando 8 clusters\n",
        "*   DBSCAN con épsilon por defecto\n",
        "*   DBSCAN con épsilon 0.7\n",
        "*   DBSCAN con épsilon 0.2\n",
        "*   DBSCAN con épsilon por defecto, agregando outliers a cluster extra\n",
        "*   DBSCAN con épsilon 0.7, agregando outliers a cluster extra\n",
        "*   DBSCAN con épsilon 0.2, agregando outliers a cluster extra\n",
        "*   Clustering aglomerativo, usando 8 clusters\n",
        "\n",
        "Para el entrenamiento, use las funciones bench_k_means( ) y bench_clustering2( ), las cuales serán usadas para obtener un benchmark de K-Means, DBSCAN y Clustering Aglomerativo."
      ],
      "metadata": {
        "id": "MqJmZIt7-JYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Paso previo cargamos \"digits dataset\" como lo indican en las instrucciones\n",
        "\n",
        "labels = np.array(data['class'])\n",
        "new_columns = []\n",
        "for i in data.columns:\n",
        "  if i == 'class':\n",
        "    break\n",
        "  new_columns.append(i)\n",
        "data = np.array(data[new_columns])\n"
      ],
      "metadata": {
        "id": "ttVR33wH3NjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Puede usar los metodos KMeans, DBSCAN, AgglomerativeClustering previamente cargados de la libreria sklearn.cluster\n",
        "\n",
        "print(90*'_')\n",
        "print('Nombre\\t\\tinit\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhoutte')\n",
        "\n",
        "#K-Means (con inicialización al azar), usando 8 clusters\n",
        "kmeans = KMeans(init= 'random', n_clusters=8, n_init=4, random_state=0)\n",
        "bench_k_means(kmeans=kmeans, name=\"k-means_Random\", data=data, labels=labels)\n",
        "\n",
        "#K-Means++, usando 8 clusters\n",
        "kmeansplus = KMeans(init= 'k-means++', n_clusters=8, n_init=4, random_state=0).fit(data)\n",
        "bench_k_means(kmeans=kmeansplus, name=\"k-means_++\", data=data, labels=labels)\n",
        "\n",
        "#DBSCAN con épsilon por defecto\n",
        "dbscan1 = DBSCAN().fit(data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN Def', data= data, labels=labels, use_outliers= False)\n",
        "#DBSCAN con épsilon 0.7\n",
        "dbscan1 = DBSCAN(eps=0.7).fit(data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.7', data= data, labels=labels, use_outliers= False)\n",
        "#DBSCAN con épsilon 0.2\n",
        "dbscan1 = DBSCAN(eps=0.2).fit(data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.2', data= data, labels=labels, use_outliers= False)\n",
        "#DBSCAN con épsilon por defecto, agregando outliers a cluster extra\n",
        "dbscan1 = DBSCAN().fit(data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN Def CE', data= data, labels=labels, use_outliers= True)\n",
        "#DBSCAN con épsilon 0.7, agregando outliers a cluster extra\n",
        "dbscan1 = DBSCAN(eps=0.7).fit(data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.7 CE', data= data, labels=labels, use_outliers= True)\n",
        "#DBSCAN con épsilon 0.2, agregando outliers a cluster extra\n",
        "dbscan1 = DBSCAN(eps=0.2).fit(data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.2 CE', data= data, labels=labels, use_outliers= True)\n",
        "#Clustering aglomerativo, usando 8 clusters\n",
        "agglo = AgglomerativeClustering(n_clusters=8).fit(data)\n",
        "bench_clustering2(estim=agglo, name='Agglomerative', data= data, labels=labels, use_outliers= False)\n",
        "\n",
        "print(90*'_')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7lbmkdwM-Imi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15a0f95-66ae-49bb-966c-2daed2e1f10e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________________________________________________\n",
            "Nombre\t\tinit\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhoutte\n",
            "k-means_Random\t8\t0.033s\t1127\t0.280\t0.282\t0.281\t0.130\t0.266\t0.209\n",
            "k-means_++\t8\t0.214s\t1124\t0.310\t0.311\t0.310\t0.148\t0.296\t0.210\n",
            "DBSCAN Def\t12\t0.009s\t-1\t1.000\t0.705\t0.827\t0.539\t0.771\t0.693\n",
            "DBSCAN.7 \t43\t0.011s\t-1\t0.975\t0.536\t0.692\t0.263\t0.630\t0.373\n",
            "DBSCAN.2 \t0\t0.011s\t-1\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\n",
            "DBSCAN Def CE\t13\t0.011s\t-1\t0.107\t0.365\t0.165\t0.002\t0.113\t-0.349\n",
            "DBSCAN.7 CE\t44\t0.011s\t-1\t0.579\t0.425\t0.490\t0.072\t0.422\t-0.006\n",
            "DBSCAN.2 CE\t1\t0.006s\t-1\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\n",
            "Agglomerative\t8\t0.020s\t-1\t0.352\t0.356\t0.354\t0.168\t0.341\t0.159\n",
            "__________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2 & 4\n",
        "\n",
        "Reduzca la base de datos cargada y pre-procesada a 2 dimensiones, usando PCA\n",
        "\n",
        "Entrenar los siguientes algoritmos de clustering, con los parámetros especificados:\n",
        "    \n",
        "*   K-Means (con inicialización al azar), usando 8 clusters\n",
        "*   K-Means++, usando 8 clusters\n",
        "*   DBSCAN con épsilon por defecto\n",
        "*   DBSCAN con épsilon 0.7\n",
        "*   DBSCAN con épsilon 0.2\n",
        "*   DBSCAN con épsilon por defecto, agregando outliers a cluster extra\n",
        "*   DBSCAN con épsilon 0.7, agregando outliers a cluster extra\n",
        "*   DBSCAN con épsilon 0.2, agregando outliers a cluster extra\n",
        "*   Clustering aglomerativo, usando 8 clusters\n",
        "\n",
        "Para el entrenamiento, use las funciones bench_k_means( ) y bench_clustering2( ), las cuales serán usadas para obtener un benchmark de K-Means, DBSCAN y Clustering Aglomerativo."
      ],
      "metadata": {
        "id": "MLo1QhcU_KaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reducir datos con PCA a dos dimensiones\n",
        "reduce_data = PCA(n_components=2).fit_transform(data)\n"
      ],
      "metadata": {
        "id": "QZYFCA2ns_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Puede usar los metodos PCA de la libreria sklearn.decomposition y KMeans, DBSCAN, AgglomerativeClustering de la libreria sklearn.cluster\n",
        "\n",
        "print(90*'_')\n",
        "print('Nombre\\t\\tinit\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhoutte')\n",
        "\n",
        "#K-Means (con inicialización al azar), usando 8 clusters\n",
        "kmeans = KMeans(init= 'random', n_clusters=8).fit(reduce_data)\n",
        "bench_k_means(kmeans=kmeans, name=\"k-means_Random\", data=reduce_data, labels=labels)\n",
        "\n",
        "#K-Means++, usando 8 clusters\n",
        "kmeansplus = KMeans(init= 'k-means++', n_clusters=8).fit(reduce_data)\n",
        "bench_k_means(kmeans=kmeansplus, name=\"k-means_++\", data=reduce_data, labels=labels)\n",
        "\n",
        "#DBSCAN con épsilon por defecto\n",
        "dbscan1 = DBSCAN().fit(reduce_data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN Def', data=reduce_data, labels=labels, use_outliers= False)\n",
        "#DBSCAN con épsilon 0.7\n",
        "dbscan1 = DBSCAN(eps=0.7).fit(reduce_data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.7', data=reduce_data, labels=labels, use_outliers= False)\n",
        "#DBSCAN con épsilon 0.2\n",
        "dbscan1 = DBSCAN(eps=0.2).fit(reduce_data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.2', data=reduce_data, labels=labels, use_outliers= False)\n",
        "#DBSCAN con épsilon por defecto, agregando outliers a cluster extra\n",
        "dbscan1 = DBSCAN().fit(reduce_data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN Def CE', data=reduce_data, labels=labels, use_outliers= True)\n",
        "#DBSCAN con épsilon 0.7, agregando outliers a cluster extra\n",
        "dbscan1 = DBSCAN(eps=0.7).fit(reduce_data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.7 CE', data=reduce_data, labels=labels, use_outliers= True)\n",
        "#DBSCAN con épsilon 0.2, agregando outliers a cluster extra\n",
        "dbscan1 = DBSCAN(eps=0.2).fit(reduce_data)\n",
        "bench_clustering2(estim=dbscan1, name='DBSCAN.2 CE', data=reduce_data, labels=labels, use_outliers= True)\n",
        "#Clustering aglomerativo, usando 8 clusters\n",
        "agglo = AgglomerativeClustering(n_clusters=8).fit(reduce_data)\n",
        "bench_clustering2(estim=agglo, name='Agglomerative', data=reduce_data, labels=labels, use_outliers= False)\n",
        "\n",
        "\n",
        "print(90*'_')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0UHWdH9i_IXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bec02e-45dc-43c9-ee18-f6f8b4376225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________________________________________________\n",
            "Nombre\t\tinit\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhoutte\n",
            "k-means_Random\t8\t0.040s\t347\t0.209\t0.212\t0.211\t0.096\t0.194\t0.366\n",
            "k-means_++\t8\t0.033s\t345\t0.211\t0.215\t0.213\t0.094\t0.197\t0.369\n",
            "DBSCAN Def\t1\t0.006s\t-1\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\n",
            "DBSCAN.7 \t1\t0.007s\t-1\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\t-1.000\n",
            "DBSCAN.2 \t20\t0.005s\t-1\t0.284\t0.283\t0.284\t0.068\t0.212\t0.083\n",
            "DBSCAN Def CE\t2\t0.009s\t-1\t0.006\t0.116\t0.011\t0.000\t0.005\t0.374\n",
            "DBSCAN.7 CE\t2\t0.007s\t-1\t0.005\t0.335\t0.010\t0.000\t0.005\t0.505\n",
            "DBSCAN.2 CE\t21\t0.005s\t-1\t0.195\t0.202\t0.198\t0.037\t0.143\t-0.205\n",
            "Agglomerative\t8\t0.013s\t-1\t0.219\t0.226\t0.223\t0.102\t0.206\t0.354\n",
            "__________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outputs:\n",
        "\n",
        "Nombre, N° Clusters, tiempo, Inercia, Homogeneidad, Completitud, Medida V, rand_Score, info_score, Puntuación de Silueta"
      ],
      "metadata": {
        "id": "ED1hgC2T1aEH"
      }
    }
  ]
}